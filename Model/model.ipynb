{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import os\n",
    "import soundfile as sf\n",
    "import torch, torchaudio\n",
    "import torchaudio.functional as f\n",
    "import re\n",
    "from scipy.signal import resample\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# กำหนด path ตามโครงสร้างโฟลเดอร์ของคุณ\n",
    "dir = '/content/drive/My Drive/archive/recordings/recordings'\n",
    "\n",
    "import os\n",
    "audio_list = os.listdir(dir)\n",
    "print('Dataset length:', len(audio_list))\n",
    "\n",
    "# ตรวจสอบว่าได้ไฟล์มาถูกต้องไหม\n",
    "print('\\nFirst few files:')\n",
    "print(audio_list[:5])  # แสดง 5 ไฟล์แรก\n",
    "df = pd.DataFrame()\n",
    "df['speech'] = audio_list\n",
    "\n",
    "labels = [re.sub(r'\\d+\\.mp3$', '', audio) for audio in audio_list]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'speech': audio_list,\n",
    "    'labels': labels\n",
    "})\n",
    "\n",
    "df.head()\n",
    "\n",
    "# count the all label\n",
    "df['labels'].value_counts()\n",
    "# data analysis of all label\n",
    "sns.countplot(x = df['labels'], data=df)\n",
    "\n",
    "target_sample_rate = 44100  # Target sample rate for the audio\n",
    "\n",
    "def extract_features(audio_file, n_fft=128, hop_length=32, n_mfcc=13):\n",
    "    # Read the audio file using soundfile library\n",
    "    y, sr = sf.read(audio_file)\n",
    "\n",
    "    # Check if the sample rate is different from the target sample rate\n",
    "    if sr != target_sample_rate:\n",
    "        # Resample the audio to the target sample rate\n",
    "        y = resample(y, int(len(y) * target_sample_rate / sr))\n",
    "        sr = target_sample_rate\n",
    "\n",
    "    # Extract the first 10 seconds of audio (assuming the file is long enough)\n",
    "    samples_10_sec = target_sample_rate * 5   # 10 seconds of audio at the target sample rate\n",
    "    y_10_sec = y[:samples_10_sec]  # Slice the audio to get the first 10 seconds\n",
    "\n",
    "    # Extract Mel-Frequency Cepstral Coefficients (MFCCs) from the audio\n",
    "    mfccs = librosa.feature.mfcc(y = y_10_sec, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "    # Normalize the MFCCs (mean = 0, standard deviation = 1)\n",
    "    mfccs_normalized = (mfccs - np.mean(mfccs)) / np.std(mfccs)\n",
    "\n",
    "    return mfccs  # Return the MFCC features\n",
    "    data = []  # Empty list to store audio features and labels\n",
    "\n",
    "# Loop through each audio file in the audio_list\n",
    "for audio in audio_list:\n",
    "    # Create the full path to the audio file\n",
    "    audio_path = dir + \"/\" + audio\n",
    "\n",
    "    # Extract features from the audio file (using the extract_features function)\n",
    "    feature = extract_features(audio_path)\n",
    "\n",
    "    # Create the label by removing numbers from the file name and removing the .mp3 extension\n",
    "    label = re.sub(r'\\d+', '', audio[:-4])  # Remove digits from the file name using regex\n",
    "\n",
    "    # Append the feature and label as a tuple to the data list\n",
    "    data.append((feature, label))\n",
    "\n",
    "    separated_data = []  # List to store dictionaries with MFCC features for each audio\n",
    "label_arr = []  # List to store the labels for each audio file\n",
    "\n",
    "# Loop through each feature-label pair in the data\n",
    "for f, l in data:\n",
    "    # Create a dictionary of MFCCs by calculating the mean of each MFCC coefficient across time\n",
    "    # The dictionary key is the MFCC name (e.g., 'MFCC_1', 'MFCC_2', etc.)\n",
    "    mfcc_dict = {f'MFCC_{i+1}': np.mean(f[i]) for i in range(f.shape[0])}\n",
    "\n",
    "    # Append the label to the label_arr list\n",
    "    label_arr.append(l)\n",
    "\n",
    "    # Append the dictionary of MFCCs to the separated_data list\n",
    "    separated_data.append(mfcc_dict)\n",
    "\n",
    "    # Convert the list of MFCC dictionaries into a Pandas DataFrame\n",
    "df_new = pd.DataFrame(separated_data)\n",
    "\n",
    "# Add the labels as a new column in the DataFrame\n",
    "df_new['label'] = label_arr\n",
    "\n",
    "# Display the first few rows of the DataFrame to check the data\n",
    "df_new.head()\n",
    "\n",
    "# Print the length of the DataFrame before removing missing values\n",
    "print(len(df_new))\n",
    "\n",
    "# Remove rows with missing (null) values from the DataFrame\n",
    "df_cleaned = df_new.dropna()\n",
    "\n",
    "# Print the length of th\n",
    "\n",
    "# Get the count of each unique value (accent label) in the 'label' column\n",
    "accent_counts = df_cleaned['label'].value_counts()\n",
    "\n",
    "# Print the counts of each accent label\n",
    "print(accent_counts)\n",
    "\n",
    "# Find the accent label with the most samples (largest group)\n",
    "largest_accent = accent_counts.idxmax()\n",
    "\n",
    "# Get the count of the largest group (most samples)\n",
    "max_count = accent_counts.max()\n",
    "\n",
    "# Initialize an empty list to store the oversampled data\n",
    "oversampled_data = []\n",
    "\n",
    "# Loop through each accent label and its count\n",
    "for accent, count in accent_counts.items():\n",
    "    # If the accent is not the largest group and has more than 30 samples\n",
    "    if accent != largest_accent and count > 30:\n",
    "        # Get the data for the current accent\n",
    "        accent_data = df_cleaned[df_cleaned['label'] == accent]\n",
    "\n",
    "        # Oversample the accent group to match the size of the largest group\n",
    "        oversampled_accent = accent_data.sample(n=max_count, replace=True, random_state=42)\n",
    "\n",
    "        # Append the oversampled data for this accent\n",
    "        oversampled_data.append(oversampled_accent)\n",
    "    # If the accent is the largest group, do not oversample, just append it as is\n",
    "    elif accent == largest_accent:\n",
    "        oversampled_data.append(df_cleaned[df_cleaned['label'] == accent])\n",
    "\n",
    "# Combine the oversampled data to form a new balanced DataFrame\n",
    "balanced_df = pd.concat(oversampled_data)\n",
    "# Shuffle the rows of the balanced DataFrame to randomize the order of the samples\n",
    "df_shuffled = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows of the shuffled DataFrame\n",
    "df_shuffled.head()\n",
    "# Separate the target variable (label) into y\n",
    "y = df_shuffled[\"label\"]\n",
    "\n",
    "# Separate the features (all columns except 'label') into x\n",
    "x = df_shuffled.drop('label', axis=1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder to encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the target labels (y) into encoded labels\n",
    "encoded_labels = label_encoder.fit_transform(y)\n",
    "\n",
    "# Create a dictionary to map original labels to encoded labels\n",
    "label_mapping = {label: encoded_label for label, encoded_label in zip(y, encoded_labels)}\n",
    "\n",
    "# Print the mapping of labels to encoded values\n",
    "class_label = []  # Initialize an empty list to store the original labels\n",
    "print(\"Label mapping:\")\n",
    "for label, encoded_label in label_mapping.items():\n",
    "    class_label.append(label)  # Add original label to the list\n",
    "    print(f\"{label}: {encoded_label}\")  # Print the original label and its corresponding encoded value\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the feature columns (excluding 'label') from the shuffled DataFrame into a NumPy array\n",
    "X = np.array(df_shuffled.drop('label', axis=1).values)\n",
    "\n",
    "# Extract the target labels ('label' column) from the shuffled DataFrame into a NumPy array\n",
    "y = df_shuffled['label'].values\n",
    "\n",
    "# Initialize the LabelEncoder to convert categorical labels into numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder to the labels and transform the labels into numeric values (encoded labels)\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert the numeric labels into one-hot encoded format (binary matrix representation)\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
    "# The features (X) and one-hot encoded labels (y_onehot) are split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# Function to build the CNN model for accent classification\n",
    "def build_cnn_model(input_shape):\n",
    "    # Initialize the Sequential model (a linear stack of layers)\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the first 1D convolutional layer with 32 filters, a kernel size of 3, and ReLU activation function\n",
    "    # input_shape is the shape of each input sample (number of features, 1 for a single channel)\n",
    "    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "\n",
    "    # Add a MaxPooling layer with pool size of 2 to downsample the output of the previous layer\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    # Add the second 1D convolutional layer with 64 filters and kernel size of 3, using ReLU activation\n",
    "    model.add(Conv1D(64, 3, activation='relu'))\n",
    "\n",
    "    # Add another MaxPooling layer to further downsample the feature map\n",
    "    model.add(MaxPooling1D(2))\n",
    "\n",
    "    # Flatten the 2D output from the last MaxPooling layer into a 1D vector\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add a fully connected (Dense) layer with 128 neurons and ReLU activation function\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    # Add Dropout layer to reduce overfitting by randomly setting 50% of the input units to zero during training\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Add the final output layer with as many neurons as there are classes (y_train.shape[1]) and softmax activation\n",
    "    # Softmax is used for multi-class classification, converting the outputs into probabilities\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "    # Compile the model with Adam optimizer, categorical crossentropy loss function, and accuracy metric\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Return the built model\n",
    "    return model\n",
    "\n",
    "# Define the input shape for the model: (number of features, 1) for a single channel (e.g., MFCC features)\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "\n",
    "# Build the CNN model using the defined input shape\n",
    "model = build_cnn_model(input_shape)\n",
    "\n",
    "# Train the model using the training data (X_train, y_train) and validate it using the test data (X_test, y_test)\n",
    "# The model will train for 150 epochs with a batch size of 32, and display progress at the end of each epoch\n",
    "history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the metrics from the history object\n",
    "metrics = history.history\n",
    "\n",
    "# Create a figure with smaller subplots placed side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))  # 1 row, 2 columns\n",
    "\n",
    "# Plot Accuracy\n",
    "axs[0].plot(history.epoch, 100 * np.array(metrics['accuracy']), 100 * np.array(metrics['val_accuracy']))\n",
    "axs[0].set_title(\"Accuracy CNN\")\n",
    "axs[0].set_ylim([0, 100])\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "axs[0].legend(['accuracy', 'val_accuracy'])\n",
    "\n",
    "# Plot Loss\n",
    "axs[1].plot(history.epoch, np.array(metrics['loss']), np.array(metrics['val_loss']))\n",
    "axs[1].set_title(\"Loss CNN\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "axs[1].legend(['loss', 'val_loss'])\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "import numpy as np\n",
    "\n",
    "# Get the predicted probabilities for each class from the model\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# Convert the predicted probabilities into class labels by selecting the index with the highest probability\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "# Convert the one-hot encoded true labels (y_test) into class labels by selecting the index with the 1 in each one-hot vector\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the list of class names corresponding to the labels\n",
    "class_names = [\n",
    "    \"Arabic\", \"Dutch\", \"English\", \"French\", \"German\",\n",
    "    \"Italian\", \"Korean\", \"Mandarin\", \"Polish\", \"Portuguese\",\n",
    "    \"Russian\", \"Spanish\", \"Turkish\"\n",
    "]\n",
    "\n",
    "# Generate the classification report, which provides metrics like precision, recall, f1-score\n",
    "# for each class in the dataset\n",
    "report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "\n",
    "# Print the classification report to see the evaluation results\n",
    "print(report)\n",
    "# Save the trained model to a file\n",
    "model.save('cnn.h5')\n",
    "\n",
    "# Load the saved model from the file\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('cnn.h5')\n",
    "model.save('cnn_model.keras')  # TensorFlow will use the SavedModel format by default\n",
    "\n",
    "import os\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import HyperModel\n",
    "import tensorflow as tf\n",
    "from joblib import load\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define base paths\n",
    "BASE_PATH = 'drive/MyDrive'\n",
    "MODEL_PATH = os.path.join(BASE_PATH, 'models')\n",
    "TUNING_PATH = os.path.join(BASE_PATH, 'model_tuning')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(TUNING_PATH, exist_ok=True)\n",
    "\n",
    "class CNNHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv1D(\n",
    "            filters=hp.Int('filters1', min_value=32, max_value=128, step=32),\n",
    "            kernel_size=hp.Int('kernel_size1', min_value=2, max_value=5, step=1),\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            input_shape=self.input_shape\n",
    "        ))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "        model.add(Conv1D(\n",
    "            filters=hp.Int('filters2', min_value=64, max_value=256, step=32),\n",
    "            kernel_size=hp.Int('kernel_size2', min_value=2, max_value=5, step=1),\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        ))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.add(Dense(\n",
    "            hp.Int('dense_units', min_value=64, max_value=256, step=64),\n",
    "            activation='relu'\n",
    "        ))\n",
    "\n",
    "        model.add(Dropout(hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "            ),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopper = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "input_shape = (X_train.shape[1], 1)\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "hypermodel = CNNHyperModel(input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=2,\n",
    "    directory=TUNING_PATH,\n",
    "    project_name='AccentClassification'\n",
    ")\n",
    "\n",
    "tuner.search(x=X_train, y=y_train, epochs=50, validation_data=(X_test, y_test),\n",
    "             callbacks=[early_stopper, reduce_lr])\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Best model accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(filters=32, kernel_size=4, activation='relu', padding='same',\n",
    "                    input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0011876),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "best_model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "history = best_model.fit(X_train, y_train, epochs=150, batch_size=32,\n",
    "                        validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Plotting\n",
    "metrics = history.history\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axs[0].plot(history.epoch, 100 * np.array(metrics['accuracy']),\n",
    "            100 * np.array(metrics['val_accuracy']))\n",
    "axs[0].set_title(\"Accuracy CNN\")\n",
    "axs[0].set_ylim([0, 100])\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "axs[0].legend(['accuracy', 'val_accuracy'])\n",
    "\n",
    "axs[1].plot(history.epoch, np.array(metrics['loss']), np.array(metrics['val_loss']))\n",
    "axs[1].set_title(\"Loss CNN\")\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "axs[1].legend(['loss', 'val_loss'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_probs = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "class_names = [\n",
    "    \"Arabic\", \"Dutch\", \"English\", \"French\", \"German\",\n",
    "    \"Italian\", \"Korean\", \"Mandarin\", \"Polish\", \"Portuguese\",\n",
    "    \"Russian\", \"Spanish\", \"Turkish\"\n",
    "]\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "print(report)\n",
    "\n",
    "# Save models\n",
    "model_h5_path = os.path.join(MODEL_PATH, 'cnn_tunning.h5')\n",
    "model_keras_path = os.path.join(MODEL_PATH, 'cnn_tunning_keras.keras')\n",
    "\n",
    "best_model.save(model_h5_path)\n",
    "best_model.save(model_keras_path)\n",
    "\n",
    "# Load and verify model\n",
    "loaded_model = tf.keras.models.load_model(model_h5_path)\n",
    "loss, accuracy = loaded_model.evaluate(X_test, y_test)\n",
    "print(\"Loaded model accuracy:\", accuracy)\n",
    "\n",
    "def preprocess_audio(file_path, target_sample_rate=44100, n_mfcc=13):\n",
    "    y, sr = librosa.load(file_path, sr=target_sample_rate)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n",
    "    mfcc = mfcc.T\n",
    "\n",
    "    if mfcc.shape[0] > 13:\n",
    "        mfcc = mfcc[:13, :]\n",
    "    else:\n",
    "        mfcc = np.pad(mfcc, ((0, 13 - mfcc.shape[0]), (0, 0)), mode='constant')\n",
    "    mfcc = np.mean(mfcc, axis=1, keepdims=True)\n",
    "    mfcc = np.expand_dims(mfcc, axis=0)\n",
    "\n",
    "    return mfcc\n",
    "\n",
    "# Function to load model\n",
    "def load_model(model_name):\n",
    "    if model_name.endswith('.h5'):\n",
    "        return tf.keras.models.load_model(model_name)\n",
    "    else:\n",
    "        return load(f'{model_name}.joblib')\n",
    "\n",
    "# Example usage\n",
    "audio_file_path = os.path.join(BASE_PATH, 'audio', 'test.mp3')  # Update with your audio path\n",
    "audio_data = preprocess_audio(audio_file_path)\n",
    "print(\"Shape of preprocessed audio data:\", audio_data.shape)\n",
    "\n",
    "model = load_model(model_h5_path)\n",
    "predictions = model.predict(audio_data)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(f\"The predicted class for the audio file is: {class_names[predicted_class[0]]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
